To set up and run the AI Chatbot project described in the README.md, you need to meet the following requirements. These are derived from the project's prerequisites and installation instructions, ensuring you have everything needed to successfully deploy and use the application.

### Requirements for AI Chatbot Project

#### Software Requirements
- **Python**: Version 3.8 or higher
  - Required for running the Streamlit application and handling dependencies.
- **Ollama Server**: 
  - Must be installed and running locally on port 11434.
  - Provides local AI processing for language models (llama2, mistral, codellama).
  - Installation instructions available at [Ollama's official documentation](https://ollama.ai/).

#### Python Dependencies
- **Streamlit**: 
  - Used to create the web-based user interface.
  - Install via: `pip install streamlit`
- **Requests**: 
  - Handles API calls to the Ollama server.
  - Install via: `pip install requests`

#### Hardware Requirements
- **Operating System**: Compatible with Python and Ollama (Windows, macOS, or Linux).
- **Memory**: Minimum 4GB RAM (8GB or more recommended for smooth performance with Ollama models).
- **Storage**: At least 1GB free disk space for Ollama models and Python dependencies.
- **Internet Connection**: Required for initial package installation and downloading Ollama models.

#### Installation Steps
1. **Install Python**:
   - Ensure Python 3.8+ is installed. Verify with: `python --version` or `python3 --version`.
2. **Install Ollama**:
   - Download and install Ollama following the official guide.
   - Start the server: `ollama serve`.
3. **Install Python Dependencies**:
   ```bash
   pip install streamlit requests
   ```
4. **Clone the Project**:
   ```bash
   git clone <repository-url>
   cd project_directory
   ```

#### Running the Application
- Launch the Streamlit app:
  ```bash
  streamlit run app.py
  ```
- Access the application in a web browser at `http://localhost:8501`.

#### Additional Notes
- Ensure the Ollama server is running before starting the application.
- Verify that the supported models (llama2, mistral, codellama) are available in Ollama.
- Monitor system resources, as running local AI models can be computationally intensive.

These requirements ensure the AI Chatbot project runs smoothly, providing a functional gym-focused chatbot with a Streamlit interface and Ollama-powered AI responses.
